\documentclass[11pt, fleqn]{article}

\usepackage[letterpaper, margin=0.7in]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{listings}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{bm}
\usepackage{minted}
\usetikzlibrary{arrows}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[section]{placeins}
\usepackage{pgfplots,wrapfig}  
\pgfplotsset{compat=newest} 
\pgfplotsset{plot coordinates/math parser=false} 
\newlength\figureheight 
\newlength\figurewidth 
\setlength{\parindent}{0pt}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\let\oldReturn\Return
\renewcommand{\Return}{\State\oldReturn}

\raggedbottom

\begin{document}

\begin{center}

\Large{COMP6212 Assignment 4 : Shakib-Bin Hamid 25250094 sh3g12}

\end{center}

\section{Kalman Filter}

Lets suppose there a data generative process is a linear of its past values, i.e. the current process process parameters are a linear combination of previous process parameters. In this respect we can calculate the process parameters by minimising the least squared error between the prediction and the true values, $\bm{\omega} = (\bm{Y}^t\bm{Y})^{-1}\bm{Y}^t\bm{f}$, where $\bm{Y}$ is the input data and $\bm{f}$ is the target. But this method requires calculating a new $\bm{Y}^{-1}$ (a $O(n^3)$ operation) when we receive any new data. If the process is generating a time series data (getting data one at a time), then online updating the parameters can be incredibly costly.\\

However, it can be shown that we can calcuate new parameters without calculating a new inverse, $\bm{\omega}_n = \bm{\omega}_{n-1} + L \times (f_n - \bm{\omega}_{n-1}^ty_n)$. It means that we can calculate new parameters by updating the old parameters with the scaled prediction error on new data using the old parameters. Of course we need to assume some initial values for our parameters. But, we still do not account for any noise in our process.\\

In a Kalman Filter, we may assume a process $\bm{\omega}_n = \bm{A}\bm{\omega}_{n-1} + \bm{\epsilon}(n)$, where the process noise $\bm{\epsilon} \sim \mathcal{N}(0, \bm{Q})$. Here we can assume that our parameters are a linear transformation of previous parameters corrupted by Gaussian noise. Similarly, we can assume that our observation $f_n = \bm{\omega}_n\bm{y}_n + \xi(n)$, where observation noise $\xi \sim \mathcal{N}(0, R)$.\\

\cite{mahler} claims that we can predict the monthly S\&P 500 index using a Kalman filter. In order to verify his claim, I have built Algorithm \ref{alg:kalman} -

\begin{algorithm}[H]
\caption{S\&P 500 Index Prediction using a Kalman Filter}
\label{alg:kalman}
\begin{algorithmic}

\Procedure{Kalman}{$\bm{s}, o, \alpha$} \Comment{$\bm{s}$ = index, $o$ = order, $\alpha$ = is a constant}

\State

\State $N$ = $size(\bm{s}, 1)$; \Comment{Number of months}
\State $m$ = $ar(s, o)$; \Comment{Matlab's autoregression function}
\State $R$ = $m.NoiseVariance$; \Comment{Observation noise variance}
\State $\bm{Y}$ = $zeros(N - o + 1, o)$; \Comment{Input data in $o$ order windows}
\State $\bm{W}$ = $zeros(N, o)$; \Comment{Weights at every point}
\State $\bm{w}$ = $(m.a(2:o+1) \times -1)^t$; \Comment{Current weight}
\State $\bm{e}$ = $zeros(N - o + 1, 1)$; \Comment{Prediction error at every point}

\State

\State $\bm{A}$ = $\bm{I}_o$ \Comment{$\bm{I}_o$ is the $o$ order Identity}
\State $\bm{Q}$ = $\alpha\bm{I}_o$ \Comment{Process noise covariance}
\State $\bm{P}$ = $I_o$ \Comment{Inital value of $\bm{P}$}

\State

\For {$n \gets o + 1,N$}

	\State $\bm{y}_n$ = $\bm{Y}(n, :)$ = $\bm{s}(n - o:n - 1)$ \Comment{Input is last $o$ months' index}

	\State

	\State $\bm{P}$ = $\bm{A}\bm{P}\bm{A}^t$ + $\bm{Q}$ \Comment{Uncertainity on parameters increases}

	\State
	
	\State $\bm{e}(n)$ = $\bm{s}(n)$ - $\bm{w}^t\bm{y}_n$ \Comment{Prediction Error}
	\State $\bm{K}$ = $\bm{P}\bm{y}_n/(R + \bm{y}_n^t\bm{P}\bm{y}_n)$ \Comment{Kalman Gain}

	\State

	\State $\bm{w} = \bm{W}(n, :)$ = $\bm{w} + \bm{K}\bm{e}(n)$ \Comment{Correction}
	\State $\bm{P}$ = $(\bm{I}_o - \bm{K}\bm{y}_n^t)\bm{P}$ 

\EndFor

\EndProcedure
\end{algorithmic}
\end{algorithm}

Here the input at each point in time is past $o \in {2, ..., N-1}$ index values. Observation noise $R$ is taken as the residual variance from an autoregression model using \texttt{ar} function of Matlab's System Identification Toolbox. Process noise covariance needed to be tuned via $\alpha$. From Figure \ref{fig:error-vs-alpha-order-3} (where I have plotted the cumulative sum of prediction error as a function of $\alpha$ for a 3rd order filter), we can say that $10^{-3}$ is a reasonable value for $\alpha$. Unless otherwise specified, it can be assumed that a 3rd order filter and autoregression was in use. The weights were initialised in $1/o$ method.\\

\begin{wrapfigure}{!h}{0.5\textwidth}
	\vspace{-1cm}
  	\centering
  	\includegraphics[width=0.5\textwidth]{kalman-autoreg-pred.png}
	\caption{Kalman \& Autoregression Prediction}
	\label{fig:kalman-autoreg-pred}
\end{wrapfigure}

Figure \ref{fig:kalman-autoreg-pred} is showing the monthly index values, as well as prediction made from an autoregression and the implemented Kalman filter. Clearly both are very close to the true values of the index. We can also see from Figure \ref{fig:kalman-autoreg-error} that both methods produce very similar error patterns. Figure \ref{fig:kalman-autoreg-error-boxplot} shows that the errors have very similar spread. The Kalman filter has a few more outliers.\\

Now we can also observe from Figure \ref{fig:error-vs-alpha-order-3} that the choice of $\alpha$ changes with order,$o$. This choice is rather arbritrary and needs tuning. As I mentioned earlier, I chose $\alpha$ = $10^{-3}$ for all orders, since it seemed reasonable at orders.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{kalman-autoreg-error.png}
		\caption{Index Prediction Absolute Error}
		\label{fig:kalman-autoreg-error}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.42\textwidth}
        \includegraphics[width=\textwidth]{kalman-autoreg-error-boxplot.png}
		\caption{Index Prediction Error Comparison}
		\label{fig:kalman-autoreg-error-boxplot}
    \end{subfigure}
    \caption{Prediction Error}
	\label{fig:prediction-error}
\end{figure}

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{error-vs-alpha-order-3.png}
	\caption{Cumulative Absolute Error as a Function of $\alpha$ order 3}
	\label{fig:error-vs-alpha-order-3}
    \end{subfigure}
    ~ 
	\begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{kalman-parameter-converge-artificial.png}
	\caption{Cumulative Absolute Error as a Function of $\alpha$ order 2}
	\label{fig:kalman-parameter-converge-artificial}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{kalman-parameter-converge.png}
		\caption{Index Prediction Error Comparison}
		\label{fig:kalman-parameter-converge}
    \end{subfigure}
    \caption{Cumulative Absolute Error as a function of $o$ and $\alpha$}
	\label{fig:o-alpha-error}
\end{figure}

\begin{figure}[!h]
  	\centering
  	
\end{figure}

\begin{thebibliography}{9}
\bibitem{mahler} 
N. Mahler, "Modeling the S \& P 500 index using the Kalman filter and the LagLasso," in \textit{Machine Learning for Signal Processing, 2009. MLSP 2009. IEEE International Workshop on, Sept 2009}, pp. 1â€“6.

\end{thebibliography}

\end{document}












































